Activation Function = it decides whether a neuron should be activated or not based on its input to the network

Adam Optimizer = (Adaptive moment estimation) it adjusts the learning rate for each individual parameter within a model rather than using a single global learning rate

Batch Normalization = it helps in stabilizing the training process. It enables higher learning rates and faster convergence

Backpropagation = the goal is to reduce the difference between the model's predicted output and the actual output by adjusting the weights and biases in the network

Classification = a machine learning model used for distinguishing among two or more output categories

Convolutional Neural Network (CNN) = network architecture used for image recognition and processing (due to its ability to recognize patterns in images)

Cross-Entropy = loss function to measure the difference between the predicted probability distribution and the true distribution

Data Augmentation = increasing the amount of data and diversity of data. There is no collection of new data, rather the already present data is transformed

Dropout = method where random neurons are dropped during training to prevent overfitting by ensuring the network does not rely too heavily on any one neuron

Dense Layer = a linear operation in which every input is connected to every output by a weight

Epoch = one complete pass through the entire training dataset

Feedforward Neural Network (FNN) = type of neural networks where information flows in one direction from the input to the output layers, without cycles or loops

Flattening = converting 2d image to 1d vector

Gradient Descent = optimization algorithm used to train ML models by locating the minimum values within a cost function (minimizing the cost function as much as possible)

Hyperparameter = configuration setting that control the learning process of the model (epochs, learning rate, etc.)

K-Fold Cross-Validation = it evaluates how well a machine learning model will perform on unseen data

Learning Rate = determines the step size at each iteration while moving towards a minimum of a loss function

Loss Function = it measures the difference between the prediction and the actual output, it's a crucial component in training neural networks as it provides a quantifiable metric to guide the optimization process (MeanSquaredError for regression tasks, Cross-Entropy Loss for classification tasks)
Loss Function = Regression models (predict continuous values) and Classification models (predict the output from a set of finite categorical values)

Model Overfitting = when a model gets trained with so much data, it starts learning from the noise and inaccurate data entries
Model Overfitting = can be prevented using cross-validation, data augmentation, dropout, early stopping, etc.

Model Underfitting = when a model is too simple to capture the underlying patterns in data, resulting in poor performance on both training and test data

Neural Network = interconnected nodes/neurons organized in layers to perform complex tasks

Normalization = the process of adjusting the input data to improve the performance and training stability of the model, such as scaling inputs to a standard range

Pooling Layer = a layer in CNNs that reduces the spatial dimensions of the input, helping to make the model computationally efficient and less prone to overfitting

Optimizer = helps to minimize the loss function

ReLU = an activation function that allows a model to solve nonlinear problems

Softmax = a function that provides probabilities for each possible output class

Stochastic = random probability distribution

Tensor = a tensor is a multidimensional array that represents the data in a machine learning model