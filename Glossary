Activation Function = it decides whether a neuron should be activated or not based on its input to the network

Adam Optimizer = (Adaptive moment estimation) it adjusts the learning rate for each individual parameter within a model rather than using a single global learning rate

Batch Normalization = it helps in stabilizing the training process. It enables higher learning rates and faster convergence

Backpropagation = the goal is to reduce the difference between the model's predicted output and the actual output by adjusting the weights and biases in the network

Classification = a machine learning model used for distinguishing among two or more output categories

Convolutional Neural Network (CNN) = network architecture used for image recognition and processing (due to its ability to recognize patterns in images)

Cross-Entropy = loss function to measure the difference between the predicted probability distribution and the true distribution

Data Augmentation = increasing the amount of data and diversity of data. There is no collection of new data, rather the already present data is transformed

Dropout = method where random neurons are dropped during training to prevent overfitting by ensuring the network does not rely too heavily on any one neuron

Dense Layer = a linear operation in which every input is connected to every output by a weight

Epoch = one complete pass through the entire training dataset

Feedforward Neural Network (FNN) = type of neural networks where information flows in one direction from the input to the output layers, without cycles or loops

Flattening = converting 2d image to 1d vector

Loss Function = it measures the difference between the prediction and the actual output, it's a crucial component in training neural networks as it provides a quantifiable metric to guide the optimization process (MeanSquaredError for regression tasks, Cross-Entropy Loss for classification tasks)

Optimizer = helps to minimize the loss function

Overfitting = when a model gets trained with so much data, it starts learning from the noise and inaccurate data entries

ReLU = an activation function that allows a model to solve nonlinear problems

Softmax = a function that provides probabilities for each possible output class

Stochastic = random probability distribution

Tensor = a tensor is a multidimensional array that represents the data in a machine learning model