Common activation functions include:

- ReLU: Turns negative inputs to 0 and keeps positive inputs as they are.

- Sigmoid: Squashes inputs to a range between 0 and 1, like a smooth on/off switch.

- Tanh: Squashes inputs to a range between -1 and 1, giving a bit more flexibility.
